{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Tutorial\n",
    "\n",
    "Name: Mark McCleane\n",
    "\n",
    "Student No: C00191900\n",
    "\n",
    "Module Lecturer: Dr Greg Doyle\n",
    "\n",
    "Due Date: 3rd April, 2019\n",
    "\n",
    "Module: Data Science and Artificial Intelligence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "Introduction    2\n",
    "\n",
    "Sources of useful information    2\n",
    "\n",
    "Main Body of Lesson    3\n",
    "\n",
    "What is data preprocessing?    3\n",
    "\n",
    "Why use data preprocessing?    3\n",
    "\n",
    "The six steps in Data Preprocessing    3\n",
    "    \n",
    "    Step 1 : Import the libraries    3\n",
    "    \n",
    "    Step 2 : Import the data-set    3\n",
    "    \n",
    "    Step 3 : Check out the missing values    3\n",
    "    \n",
    "    Step 4 : See the Categorical Values    3\n",
    "    \n",
    "    Step 5 : Splitting the data-set into Training and Test Set    3\n",
    "    \n",
    "    Step 6 : Feature Scaling    3\n",
    "\n",
    "Suggested Algorithms    4\n",
    "\n",
    "Conclusion    4\n",
    "\n",
    "References    5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this tutorial I will teach you data processing and a machine learning algorithm.\n",
    "\n",
    "I will teach you when you need to data preprocess, how to data preprocess and then afterwards I will guide you through an example of data preprocessing. After that I will guide you through a tutorial in a machine learning algorithm. I will also detail how useful the selected algorithm is, what the algorithm can be used for and more importantly I will detail how to use it and I will give you some diagrams on it to help better your understanding of that algorithm.\n",
    "Sources of useful information\n",
    "If I find some trouble on the journey (as you probably will), there are millions of online resources that can help you with data science problems. \n",
    "Some of these are;\n",
    "\n",
    "    Data Science Stack Exchange - If you’ve programmed before then you would definitely know what stack overflow is. Well Data Science Stack Exchange is the data science version of stack overflow. If you have any question related to data science, someone on Data Science Stack Exchange will have the answer to that question.\n",
    " \n",
    "    Youtube - If your prefered method of help is through videos,\n",
    "    then youtube is the best video site to learn data science through tutorials, lectures and other types of videos.\n",
    "    Example channel here.\n",
    " \n",
    "    Online courses such as coursera, datacamp and google machine learning crash course.\n",
    "    These courses would be a mixture of video lectures, exercises and written notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Body of Lesson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is data preprocessing?\n",
    "\n",
    "Data preprocessing is a ‘data mining technique that transforms raw data into an understandable format’[1].\n",
    "\n",
    "It is basically the technique to remove uncomplete raw data from a dataset to make it usable for a model. This could involve removing null and/or other impossible values.\n",
    "An example of this is that if you read in the weather temperature for a certain day and the temperature at a certain time was null. This is impossible as the temperature cannot be null. This data would then have to be removed or changed to not make the data invalid or void."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why use data preprocessing?\n",
    "\n",
    "As stated above we need to remove null and other impossible values from the dataset. This could be removing the field from the dataset or editing it to give it a realistic value. For example if the same weather dataset has a null value but a minute before that the temperature was 17 C it would make sense to replace that null value with 17 C as it is unlikely that the temperature would majorly change in one minute. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The six steps in Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are generally six stages of data preprocessing which are;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 : Import the libraries\n",
    "\n",
    "Import the different libraries that you require to use.\n",
    "The libraries that I would recommend you to use would be at least numpy and pandas.\n",
    "I installed them through using scikit-learn which is a collection of machine-learning library that contains multiple data science libraries.\n",
    "Numpy is a package for scientific computing and pandas is used for data manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Line 2 is importing numpy and line 3 is importing pandas.\n",
    "Line 1 is importing the Linear Regression algorithm from the sklearn linear model.\n",
    "Once these 3 libraries are imported, we can then set the model to whatever algorithm we’d like.\n",
    "In the below example, we are using linear regression, \n",
    "which is an algorithm that models a ‘target prediction value based on independent variables’.[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 : Import the data-set\n",
    "\n",
    "Here we're going to read the dataset and output the first five rows.\n",
    "\n",
    "You can download this dataset here -> https://datahub.io/sports-data/english-premier-league#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = \"/home/markcollege/Desktop/CLICK_ME/Data_Science/Individual_DS_Project/season-1819_csv.csv\"\n",
    "data = pd.read_csv(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>HomeTeam</th>\n",
       "      <th>AwayTeam</th>\n",
       "      <th>FTHG</th>\n",
       "      <th>FTAG</th>\n",
       "      <th>FTR</th>\n",
       "      <th>HTHG</th>\n",
       "      <th>HTAG</th>\n",
       "      <th>HTR</th>\n",
       "      <th>Referee</th>\n",
       "      <th>...</th>\n",
       "      <th>HST</th>\n",
       "      <th>AST</th>\n",
       "      <th>HF</th>\n",
       "      <th>AF</th>\n",
       "      <th>HC</th>\n",
       "      <th>AC</th>\n",
       "      <th>HY</th>\n",
       "      <th>AY</th>\n",
       "      <th>HR</th>\n",
       "      <th>AR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-08-10</td>\n",
       "      <td>Man United</td>\n",
       "      <td>Leicester</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>A Marriner</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-08-11</td>\n",
       "      <td>Bournemouth</td>\n",
       "      <td>Cardiff</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>K Friend</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-08-11</td>\n",
       "      <td>Fulham</td>\n",
       "      <td>Crystal Palace</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>M Dean</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-08-11</td>\n",
       "      <td>Huddersfield</td>\n",
       "      <td>Chelsea</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>C Kavanagh</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-08-11</td>\n",
       "      <td>Newcastle</td>\n",
       "      <td>Tottenham</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>M Atkinson</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date      HomeTeam        AwayTeam  FTHG  FTAG FTR  HTHG  HTAG HTR  \\\n",
       "0  2018-08-10    Man United       Leicester     2     1   H     1     0   H   \n",
       "1  2018-08-11   Bournemouth         Cardiff     2     0   H     1     0   H   \n",
       "2  2018-08-11        Fulham  Crystal Palace     0     2   A     0     1   A   \n",
       "3  2018-08-11  Huddersfield         Chelsea     0     3   A     0     2   A   \n",
       "4  2018-08-11     Newcastle       Tottenham     1     2   A     1     2   A   \n",
       "\n",
       "      Referee ...  HST  AST  HF  AF  HC  AC  HY  AY  HR  AR  \n",
       "0  A Marriner ...    6    4  11   8   2   5   2   1   0   0  \n",
       "1    K Friend ...    4    1  11   9   7   4   1   1   0   0  \n",
       "2      M Dean ...    6    9   9  11   5   5   1   2   0   0  \n",
       "3  C Kavanagh ...    1    4   9   8   2   5   2   1   0   0  \n",
       "4  M Atkinson ...    2    5  11  12   3   5   2   2   0   0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5) #First 5 rows of dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Checking For Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date        0\n",
       "HomeTeam    0\n",
       "AwayTeam    0\n",
       "FTHG        0\n",
       "FTAG        0\n",
       "FTR         0\n",
       "HTHG        0\n",
       "HTAG        0\n",
       "HTR         0\n",
       "Referee     0\n",
       "HS          0\n",
       "AS          0\n",
       "HST         0\n",
       "AST         0\n",
       "HF          0\n",
       "AF          0\n",
       "HC          0\n",
       "AC          0\n",
       "HY          0\n",
       "AY          0\n",
       "HR          0\n",
       "AR          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from this example, this dataset has no missing values.\n",
    "\n",
    "Now if i import a similiar dataset that contains missing values, we should get missing values from dataset.isnull().sum()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date        0\n",
       "HomeTeam    1\n",
       "AwayTeam    1\n",
       "FTHG        1\n",
       "FTAG        1\n",
       "FTR         1\n",
       "HTHG        1\n",
       "HTAG        1\n",
       "HTR         1\n",
       "Referee     1\n",
       "HS          1\n",
       "AS          1\n",
       "HST         1\n",
       "AST         1\n",
       "HF          1\n",
       "AF          1\n",
       "HC          1\n",
       "AC          1\n",
       "HY          1\n",
       "AY          1\n",
       "HR          1\n",
       "AR          1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location = \"/home/markcollege/Desktop/CLICK_ME/Data_Science/Individual_DS_Project/season-1819_csv (missing_data).csv\"\n",
    "dataset = pd.read_csv(location)\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now since we got some missing values, we must remove them missing values from the dataset as it will make the dataset void. We will do this by using the dropna() function. This will remove all values that are null or empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date        0\n",
       "HomeTeam    0\n",
       "AwayTeam    0\n",
       "FTHG        0\n",
       "FTAG        0\n",
       "FTR         0\n",
       "HTHG        0\n",
       "HTAG        0\n",
       "HTR         0\n",
       "Referee     0\n",
       "HS          0\n",
       "AS          0\n",
       "HST         0\n",
       "AST         0\n",
       "HF          0\n",
       "AF          0\n",
       "HC          0\n",
       "AC          0\n",
       "HY          0\n",
       "AY          0\n",
       "HR          0\n",
       "AR          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.dropna(inplace=True)\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option that we could've used is to calculate either the mean, mode or median of the missing data and replace it with the missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: See Cathegorical Values\n",
    "\n",
    "Because all algorithms are mathemically based, we cannot have any data that is a word, so then we have to change all word values into number values. So our first word value that needs to be changed is Manchester United so we then change Manchester United into a numeric value. \n",
    "\n",
    "In order to deal with this, we are going to use the LabelEncoder algorithm from the preprocessing section of sklearn. We the use the fit_transform. In this example we are going to change the home team and the away team into a numeric value, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['2018-08-10', 'Man United', 'Leicester', ..., 2, 1, 0],\n",
       "       ['2018-08-11', 'Bournemouth', 'Cardiff', ..., 1, 1, 0],\n",
       "       ['2018-08-11', 'Fulham', 'Crystal Palace', ..., 1, 2, 0],\n",
       "       ...,\n",
       "       ['2019-03-10', 'Arsenal', 'Man United', ..., 2, 2, 0],\n",
       "       ['2019-03-10', 'Chelsea', 'Wolves', ..., 1, 4, 0],\n",
       "       ['2019-03-10', 'Liverpool', 'Burnley', ..., 2, 0, 0]], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Before Change\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "data_cathegorical = data.iloc[:,: -1].values\n",
    "data_cathegorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['2018-08-10', 13, 10, ..., 2, 1, 0],\n",
       "       ['2018-08-11', 1, 4, ..., 1, 1, 0],\n",
       "       ['2018-08-11', 8, 6, ..., 1, 2, 0],\n",
       "       ...,\n",
       "       ['2019-03-10', 0, 13, ..., 2, 2, 0],\n",
       "       ['2019-03-10', 5, 19, ..., 1, 4, 0],\n",
       "       ['2019-03-10', 11, 3, ..., 2, 0, 0]], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#After Change\n",
    "data_cathegorical[:,1] = label_encoder.fit_transform(data_cathegorical[:,1])\n",
    "data_cathegorical[:,2] = label_encoder.fit_transform(data_cathegorical[:,2])\n",
    "data_cathegorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 - Splitting the Data Sets\n",
    "\n",
    "We then need to spit the dataset into two different datasets, a training set and a test set.\n",
    "\n",
    "SciKit-learn recommends that the training set should have 70 - 80 % of the dataset and the remaining going into the test set to check for accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [2, 3],\n",
       "       [4, 5],\n",
       "       [6, 7],\n",
       "       [8, 9]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X,y = np.arange(10).reshape((5,2)), range(5)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2], [3, 4]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_split(y,shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 - Feature Scaling\n",
    "\n",
    "Feature scaling is adjusting data that has different scales to avoid biases.\n",
    "\n",
    "Examples of feature scalling is Data Normalisation and Data Standarisation.\n",
    "\n",
    "Data normalisation is changing the values of numeric columns  to a common scale without distorting the differences in the range of values.\n",
    "\n",
    "Data standarisation is obtaining  data from different sources and changing it into a generic format.\n",
    "\n",
    "They're both rescaling techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When to Scale\n",
    "\n",
    "You should normalise when the scale of the feature is irrevelant or misleading and you should not normalise when the scale is meaningful. For example K-means algorithm considers euclidean distance to be meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
